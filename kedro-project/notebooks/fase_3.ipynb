{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fase 3: Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objetivo de la fase: El objetivo de la preparación de datos es crear un conjunto de datos limpio y adecuado que permita implementar el modelo de recomendación de libros. Esto incluye la selección de las variables relevantes, el manejo de los datos faltantes, la codificación de variables categóricas, y la transformación de los datos en un formato que pueda ser utilizado en el análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import kaggle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = 'data/goodreads_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(csv_file_path):\n",
    "    kaggle.api.dataset_download_files('jishikajohari/best-books-10k-multi-genre-data', path='data/', unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_file_path, delimiter=',', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un nuevo DataFrame para trabajar\n",
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rellenar datos en la columna 'Description'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver resumen de valores faltantes\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percentage = (missing_data / len(df)) * 100\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(\"Valores faltantes por columna:\")\n",
    "print(missing_data)\n",
    "\n",
    "print(\"\\nPorcentaje de valores faltantes por columna:\")\n",
    "print(missing_percentage)\n",
    "\n",
    "df['Description'].fillna('No Description', inplace=True)\n",
    "missing_data_after = df.isnull().sum()\n",
    "print(\"\\nValores faltantes después de la imputación:\")\n",
    "print(missing_data_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rellenamos los datos faltantes de Description con \"No Description\", ya que, no es una columna que utilizaremos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminar filas en la columna 'Genres'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay un total de 960 filas en la columna 'Genre', en donde hay listas, pero no hay información de generos, lo que hemos decidido hacer en este caso es eliminar las filas, ya que, utilizaremos esta columna para la tarea de clasificación en donde tomaremos los datos de 'Genre'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar filas donde 'Genres' son listas vacías\n",
    "df = df[~df['Genres'].apply(lambda x: isinstance(x, list) and len(x) == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener el número de filas y columnas\n",
    "num_rows, num_columns = df.shape\n",
    "print(f\"Número de filas: {num_rows}, Número de columnas: {num_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset luego de la injección. Nuestro dataset pasa de 10.000 datos a 9.040."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar solo los 20 géneros más comunes\n",
    "top_genres = df['Main_Genre'].value_counts().nlargest(20).index\n",
    "\n",
    "# Filtrar el DataFrame para solo los géneros principales\n",
    "df_top_genres = df[df['Main_Genre'].isin(top_genres)]\n",
    "\n",
    "# Gráfico con los 20 géneros más comunes\n",
    "plt.figure(figsize=(10, 6))\n",
    "sb.countplot(y='Main_Genre', data=df_top_genres, order=df_top_genres['Main_Genre'].value_counts().index)\n",
    "plt.title('Distribución de los 20 Géneros Principales')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminar filas en la columna 'Author'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay tan solo 28 filas de autores que no se reconocen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar filas donde el autor es 'Anonymous'\n",
    "df = df[df['Author'] != 'Anonymous']\n",
    "\n",
    "# Confirmar que las filas han sido eliminadas\n",
    "remaining_anonymous_count = df[df['Author'] == 'Anonymous'].shape[0]\n",
    "print(f\"Número de autores 'Anonymous' después de la eliminación: {remaining_anonymous_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener el número de filas y columnas\n",
    "num_rows, num_columns = df.shape\n",
    "print(f\"Número de filas: {num_rows}, Número de columnas: {num_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creemos que debemos eliminar las filas en donde no se encuentran 'Authors' y 'Genres', ya que, son columnas primordiales para nuestra tarea de clasificación. En cambio para la columna de 'Description' preferimos rellenarlo con 'No Description', este cambio no deberia influir en nuestro futuros modelos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar los valores en la columna 'Author'\n",
    "author_counts = df['Author'].value_counts()\n",
    "\n",
    "# Obtener el conteo de 'Anonymous'\n",
    "anonymous_count = author_counts.get('Anonymous', 0)\n",
    "print(f\"Número de autores 'Anonymous': {anonymous_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisar los valores faltantes en el dataset\n",
    "missing_values = df1.isnull().sum()\n",
    "print(missing_values)\n",
    "\n",
    "# Eliminar filas con valores faltantes\n",
    "df1_clean = df1.dropna()\n",
    "\n",
    "# O, si es más conveniente, rellenar los valores faltantes\n",
    "df1_clean = df1.fillna(method='ffill')  # Forward fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar valores faltantes\n",
    "missing_data = df1.isnull().sum()\n",
    "missing_percentage = (missing_data / len(df1)) * 100\n",
    "\n",
    "print(\"Valores faltantes por columna:\")\n",
    "print(missing_data)\n",
    "print(\"\\nPorcentaje de valores faltantes por columna:\")\n",
    "print(missing_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rellenar valores faltantes en 'Description' con 'No Description'\n",
    "df1['Description'].fillna('No Description', inplace=True)\n",
    "\n",
    "# Verificar nuevamente los valores faltantes\n",
    "missing_data_after = df1.isnull().sum()\n",
    "print(\"\\nValores faltantes después de la imputación:\")\n",
    "print(missing_data_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar las variables numéricas\n",
    "scaler = MinMaxScaler()\n",
    "df1[['Avg_Rating', 'Num_Ratings']] = scaler.fit_transform(df1[['Avg_Rating', 'Num_Ratings']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir listas en cadenas separadas por comas para las columnas 'Author' y 'Genres'\n",
    "df1['Genres'] = df1['Genres'].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
    "df1['Author'] = df1['Author'].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "# Aplicar OneHotEncoding después de convertir listas en cadenas\n",
    "df1 = pd.get_dummies(df1, columns=['Author', 'Genres'], drop_first=True)\n",
    "\n",
    "# Mostrar las primeras filas para confirmar el proceso\n",
    "print(df1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminación de outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las columnas numéricas en las que queremos detectar y eliminar outliers\n",
    "numerical_columns = ['Avg_Rating', 'Num_Ratings']  # Ajusta los nombres a tus columnas numéricas\n",
    "\n",
    "# Función para detectar outliers usando el método del rango intercuartílico (IQR)\n",
    "def detect_outliers_iqr(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "# Detectamos outliers en las variables numéricas\n",
    "for column in numerical_columns:\n",
    "    outliers = detect_outliers_iqr(df1, column)\n",
    "    print(f'Outliers en {column}:')\n",
    "    print(outliers)\n",
    "\n",
    "# Función para eliminar outliers en las columnas numéricas\n",
    "def remove_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Filtramos el DataFrame eliminando los outliers\n",
    "    df_filtered = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "# Para cada columna numérica, eliminamos los outliers correspondientes\n",
    "for column in numerical_columns:\n",
    "    df1 = remove_outliers(df1, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos el DataFrame sin outliers\n",
    "print(\"Data después de eliminar outliers:\")\n",
    "print(df1.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de la eliminación de los outliers quedamos con 9.012 datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para finalizar estas etapa podemos quedarnos con: Existe una correlación entre Avg_Rating y Num_Rating, esto se puede significar a que mientras \n",
    "más calificaciones se hacen, el lector suele hacer reseñas. También las calificaciones promedio van entre 3 a 4.5\n",
    "lo que indica que los libros están bien valorado por los lectores. Hicimos una copia del dataframe para trabajarlo, eliminamos espacio extra de una columna, normalizamos variables numéricas, creamos matriz de variables numericas, agrupamos valores numericos en categorias y Al comienzo teniamos 10.000 datos y luego de la eliminación de outliers quedamos en 9.012."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos OneHotEncoder en los datos que vamos a trabajar para la tarea de regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# One-Hot Encoding para 'Main_Genre' y 'Author'\n",
    "X = pd.get_dummies(df[['Num_Ratings', 'Main_Genre', 'Author']], drop_first=True)\n",
    "y = df['Avg_Rating']\n",
    "\n",
    "# Ahora X tendrá columnas adicionales para cada categoría en 'Main_Genre' y 'Author'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir las primeras filas y la forma original del DataFrame\n",
    "print(\"DataFrame Original:\")\n",
    "print(df1.head())\n",
    "\n",
    "# Verificar el tipo de datos en las columnas 'Author' y 'Genres'\n",
    "print(\"Tipos de datos:\")\n",
    "print(df1['Author'].apply(type).value_counts())\n",
    "print(df1['Genres'].apply(type).value_counts())\n",
    "\n",
    "# Convertir listas en cadenas separadas por comas para 'Genres' y 'Author'\n",
    "df1['Genres'] = df1['Genres'].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
    "df1['Author'] = df1['Author'].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "# Imprimir las primeras filas después de la conversión\n",
    "print(\"DataFrame después de convertir listas a cadenas:\")\n",
    "print(df1.head())\n",
    "\n",
    "# Seleccionar las columnas categóricas\n",
    "categorical_cols = ['Author', 'Genres']\n",
    "\n",
    "# Aplicar OneHotEncoder y transformar los datos\n",
    "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "\n",
    "# Comprobar si las columnas existen\n",
    "if all(col in df1.columns for col in categorical_cols):\n",
    "    encoded_data = encoder.fit_transform(df1[categorical_cols])\n",
    "else:\n",
    "    print(f\"Las columnas {categorical_cols} no están en el DataFrame.\")\n",
    "\n",
    "# Convertir el resultado en un DataFrame y agregar los nombres de las nuevas columnas\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "# Unir el DataFrame original con el DataFrame de variables codificadas\n",
    "df_encoded = pd.concat([df1.reset_index(drop=True), encoded_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Eliminar las columnas categóricas originales si no se necesitan\n",
    "df_encoded = df_encoded.drop(columns=categorical_cols)\n",
    "\n",
    "# Mostrar las primeras filas para verificar el resultado\n",
    "print(\"DataFrame Codificado:\")\n",
    "print(df_encoded.head())\n",
    "print(\"Forma del DataFrame Codificado:\", df_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar las columnas categóricas\n",
    "categorical_cols = ['Author', 'Genres']\n",
    "\n",
    "# Aplicar OneHotEncoder y transformar los datos\n",
    "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "encoded_data = encoder.fit_transform(df1[categorical_cols])\n",
    "\n",
    "# Convertir el resultado en un DataFrame y agregar los nombres de las nuevas columnas\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "# Unir el DataFrame original con el DataFrame de variables codificadas\n",
    "df_encoded = pd.concat([df1.reset_index(drop=True), encoded_df], axis=1)\n",
    "\n",
    "# Eliminar las columnas categóricas originales si no se necesitan\n",
    "df_encoded = df_encoded.drop(columns=categorical_cols)\n",
    "\n",
    "# Mostrar las primeras filas para verificar el resultado\n",
    "print(df_encoded.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codificamos las variables categoricas y normalizamos los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparamos datos para el modelo de regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Num_Ratings'] = pd.to_numeric(df['Num_Ratings'], errors='coerce')  # Convertir a numérico si es necesario\n",
    "\n",
    "# Seleccionar las columnas relevantes\n",
    "features = ['Num_Ratings', 'Genres', 'Author']  # Puedes agregar otras si las consideras útiles\n",
    "target = 'Avg_Rating'  # La variable objetivo\n",
    "\n",
    "# Dividir las características (X) y la variable objetivo (y)\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Separar variables numéricas y categóricas\n",
    "numerical_features = ['Num_Ratings']\n",
    "categorical_features = ['Genres', 'Author']\n",
    "\n",
    "# Definir transformador para variables numéricas (escalado)\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())  # Estandarizar los datos\n",
    "])\n",
    "\n",
    "# Definir transformador para variables categóricas (OneHotEncoding)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # Codificación one-hot para las categóricas\n",
    "])\n",
    "\n",
    "# Crear un preprocesador que aplica diferentes transformaciones\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Aplicar el preprocesamiento al conjunto de entrenamiento\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"Datos preparados para el modelo de regresión.\")\n",
    "print(\"X_train_processed:\", X_train_processed.shape)\n",
    "print(\"X_test_processed:\", X_test_processed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizamos los datos para que todas las características tengan una escala similar, evitando que características con valores más grandes (por ejemplo, ratings_count) dominen el modelo.\n",
    "También aplicamos One-Hot Encoding a las variables categóricas (como autores) para convertirlas en variables numéricas, ya que los algoritmos de regresión no pueden manejar datos categóricos directamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir variables categóricas a variables dummy\n",
    "X = pd.get_dummies(df1[['Num_Ratings', 'Main_Genre', 'Author']], drop_first=True)\n",
    "y = df1['Avg_Rating']\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Escalar las características (opcional)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
